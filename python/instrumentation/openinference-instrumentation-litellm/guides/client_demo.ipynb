{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo to instrument LiteLLM calls and send their spans to our Phoenix collector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Phoenix, OTel, and other dependencies. \n",
    "Get your API Key from an .env file.\n",
    "Launch the Phoenix app and send the endpoint to be the Phoenix collector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "import litellm\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the secret key from environment variables\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "\n",
    "session = px.launch_app()\n",
    "\n",
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
    "\n",
    "endpoint = \"http://127.0.0.1:6006/v1/traces\"\n",
    "tracer_provider = TracerProvider()\n",
    "tracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter(endpoint)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need to run below cell if you have done ```pip install openinference-instrumentation-litellm``` already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the package path to the system path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Determine the absolute path to the 'src' directory\n",
    "package_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'src'))\n",
    "\n",
    "# Add the package path to the system path if it's not already included\n",
    "if package_path not in sys.path:\n",
    "    sys.path.append(package_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up our LiteLLM instrumentor with just 2 lines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openinference.instrumentation.litellm import LiteLLMInstrumentor\n",
    "LiteLLMInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make calls to LiteLLM functions as usual. You will see their spans in the Phoenix UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple single message completion call\n",
    "litellm.completion(model=\"gpt-3.5-turbo\", \n",
    "                   messages=[{\"content\": \"What's the capital of China?\", \"role\": \"user\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple message conversation completion call with added param\n",
    "litellm.completion(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{ \"content\": \"Hello, I want to bake a cake\",\"role\": \"user\"},\n",
    "                      { \"content\": \"Hello, I can pull up some recipes for cakes.\",\"role\": \"assistant\"},\n",
    "                      { \"content\": \"No actually I want to make a pie\",\"role\": \"user\"},],\n",
    "            temperature=0.7\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple message conversation acompletion call with added params\n",
    "await litellm.acompletion(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{ \"content\": \"Hello, I want to bake a cake\",\"role\": \"user\"},\n",
    "                      { \"content\": \"Hello, I can pull up some recipes for cakes.\",\"role\": \"assistant\"},\n",
    "                      { \"content\": \"No actually I want to make a pie\",\"role\": \"user\"},],\n",
    "            temperature=0.7,\n",
    "            max_tokens=20\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform image analysis by providing a url to the image and querying the LLM\n",
    "litellm.completion(\n",
    "    model = \"gpt-4o\", \n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"text\",\n",
    "                                \"text\": \"Whatâ€™s in this image?\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"type\": \"image_url\",\n",
    "                                \"image_url\": {\n",
    "                                \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "        }\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First run pip install tenacity\n",
    "litellm.completion_with_retries(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{ \"content\": \"What's the highest grossing film ever\",\"role\": \"user\"}]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "litellm.embedding(model='text-embedding-ada-002', input=[\"good morning from litellm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await litellm.aembedding(model='text-embedding-ada-002', input=[\"good morning from litellm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image generation using OpenAI\n",
    "litellm.image_generation(model='dall-e-2', prompt=\"cute baby otter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await litellm.aimage_generation(model='dall-e-2', prompt=\"cute baby otter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image generation using Bedrock\n",
    "# pip install boto3 first before importing\n",
    "import boto3\n",
    " \n",
    "os.getenv('AWS_ACCESS_KEY_ID')\n",
    "os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "os.getenv('AWS_SESSION_TOKEN')\n",
    "os.getenv('AWS_REGION')\n",
    "\n",
    "litellm.image_generation(model='bedrock/stability.stable-diffusion-xl-v1', prompt=\"blue sky with fluffy white clouds and green hills\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now uninstrument LiteLLM calls and see how you will no longer see traces for the calls in the following cells until instrumented again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LiteLLMInstrumentor().uninstrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "litellm.embedding(model='text-embedding-ada-002', input=[\"good morning from litellm\"])\n",
    "await litellm.acompletion(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{ \"content\": \"Hello, I want to bake a cake\",\"role\": \"user\"},\n",
    "                      { \"content\": \"Hello, I can pull up some recipes for cakes.\",\"role\": \"assistant\"},\n",
    "                      { \"content\": \"No actually I want to make a pie\",\"role\": \"user\"},],\n",
    "            temperature=0.7,\n",
    "            max_tokens=20\n",
    "        )\n",
    "litellm.completion(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{ \"content\": \"Hello, I want to bake a cake\",\"role\": \"user\"},\n",
    "                      { \"content\": \"Hello, I can pull up some recipes for cakes.\",\"role\": \"assistant\"},\n",
    "                      { \"content\": \"No actually I want to make a pie\",\"role\": \"user\"},],\n",
    "            temperature=0.7\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now instrument again, and you will see traces in the Phoenix UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LiteLLMInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "litellm.embedding(model='text-embedding-ada-002', input=[\"good morning from litellm\"])\n",
    "await litellm.acompletion(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{ \"content\": \"Hello, I want to bake a cake\",\"role\": \"user\"},\n",
    "                      { \"content\": \"Hello, I can pull up some recipes for cakes.\",\"role\": \"assistant\"},\n",
    "                      { \"content\": \"No actually I want to make a pie\",\"role\": \"user\"},],\n",
    "            temperature=0.7,\n",
    "            max_tokens=20\n",
    "        )\n",
    "litellm.completion(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{ \"content\": \"Hello, I want to bake a cake\",\"role\": \"user\"},\n",
    "                      { \"content\": \"Hello, I can pull up some recipes for cakes.\",\"role\": \"assistant\"},\n",
    "                      { \"content\": \"No actually I want to make a pie\",\"role\": \"user\"},],\n",
    "            temperature=0.7\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openinference-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
