{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/mambaforge/base/envs/openinference-dev/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:phoenix.config:üìã Ensuring phoenix working directory: /Users/shreyasridhar/.phoenix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722972431.101137  313290 config.cc:230] gRPC experiments enabled: call_status_override_on_cancellation, event_engine_dns, event_engine_listener, http2_stats_fix, monitoring_experiment, pick_first_new, trace_record_callops, work_serializer_clears_time_cache\n"
     ]
    }
   ],
   "source": [
    "import phoenix as px\n",
    "import litellm\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the secret key from environment variables\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "\n",
    "session = px.launch_app()\n",
    "\n",
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
    "\n",
    "endpoint = \"http://127.0.0.1:6006/v1/traces\"\n",
    "tracer_provider = TracerProvider()\n",
    "tracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter(endpoint)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the package path to the system path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Determine the absolute path to the 'src' directory\n",
    "package_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'src'))\n",
    "\n",
    "# Add the package path to the system path if it's not already included\n",
    "if package_path not in sys.path:\n",
    "    sys.path.append(package_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openinference.instrumentation.litellm import LiteLLMInstrumentor\n",
    "LiteLLMInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1722972442.379688  313199 work_stealing_thread_pool.cc:320] WorkStealingThreadPoolImpl::PrepareFork\n",
      "I0000 00:00:1722972442.379869  313199 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-9tK82fex466IyLwCbEI6ES8cM9hoy', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Beijing', role='assistant', tool_calls=None, function_call=None))], created=1722972442, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=Usage(completion_tokens=2, prompt_tokens=14, total_tokens=16))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple single message completion call\n",
    "litellm.completion(model=\"gpt-3.5-turbo\", \n",
    "                   messages=[{\"content\": \"What's the capital of China?\", \"role\": \"user\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-9tK8ATxcNSROS4xiYYl8xZUkbOa2k', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Great! What kind of pie are you looking to make? Apple, cherry, pumpkin, pecan, or something else? Let me know and I can help you find a recipe.', role='assistant', tool_calls=None, function_call=None))], created=1722972450, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=Usage(completion_tokens=37, prompt_tokens=42, total_tokens=79))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiple message conversation completion call with added param\n",
    "litellm.completion(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{ \"content\": \"Hello, I want to bake a cake\",\"role\": \"user\"},\n",
    "                      { \"content\": \"Hello, I can pull up some recipes for cakes.\",\"role\": \"assistant\"},\n",
    "                      { \"content\": \"No actually I want to make a pie\",\"role\": \"user\"},],\n",
    "            temperature=0.7\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1722971124.005816  300873 work_stealing_thread_pool.cc:320] WorkStealingThreadPoolImpl::PrepareFork\n",
      "I0000 00:00:1722971124.005994  300873 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-9tJmmuD3LwF0qBbeW6Gtj6IL4wpD1', choices=[Choices(finish_reason='length', index=0, message=Message(content='Great! What kind of pie are you thinking of making? I can help you find a recipe for', role='assistant', tool_calls=None, function_call=None))], created=1722971124, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=Usage(completion_tokens=20, prompt_tokens=42, total_tokens=62))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiple message conversation acompletion call with added params\n",
    "await litellm.acompletion(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{ \"content\": \"Hello, I want to bake a cake\",\"role\": \"user\"},\n",
    "                      { \"content\": \"Hello, I can pull up some recipes for cakes.\",\"role\": \"assistant\"},\n",
    "                      { \"content\": \"No actually I want to make a pie\",\"role\": \"user\"},],\n",
    "            temperature=0.7,\n",
    "            max_tokens=20\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "litellm.completion_with_retries(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{ \"content\": \"What's the highest grossing film ever\",\"role\": \"user\"}]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "litellm.embedding(model='text-embedding-ada-002', input=[\"good morning from litellm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await litellm.aembedding(model='text-embedding-ada-002', input=[\"good morning from litellm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageResponse(created=1722972470, data=[{'b64_json': None, 'revised_prompt': None, 'url': 'https://oaidalleapiprodscus.blob.core.windows.net/private/org-BbTPJCaZn15AHWabDLEhr6Zd/user-yOf3YhCIEb08AQU8TwPBEMcf/img-FUr1aFnu9Z1SZhhrlFBEkBui.png?st=2024-08-06T18%3A27%3A50Z&se=2024-08-06T20%3A27%3A50Z&sp=r&sv=2023-11-03&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2024-08-06T05%3A42%3A07Z&ske=2024-08-07T05%3A42%3A07Z&sks=b&skv=2023-11-03&sig=HmmRW1MuYTGQ9VILpJ%2BKlX/PhyfzZCjXM/KbcL74ju4%3D'}], usage={'prompt_tokens': 0, 'completion_tokens': 0, 'total_tokens': 0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "litellm.image_generation(model='dall-e-2', prompt=\"cute baby otter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await litellm.aimage_generation(model='dall-e-2', prompt=\"cute baby otter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LiteLLMInstrumentor().uninstrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "litellm.embedding(model='text-embedding-ada-002', input=[\"good morning from litellm\"])\n",
    "await litellm.acompletion(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{ \"content\": \"Hello, I want to bake a cake\",\"role\": \"user\"},\n",
    "                      { \"content\": \"Hello, I can pull up some recipes for cakes.\",\"role\": \"assistant\"},\n",
    "                      { \"content\": \"No actually I want to make a pie\",\"role\": \"user\"},],\n",
    "            temperature=0.7,\n",
    "            max_tokens=20\n",
    "        )\n",
    "litellm.completion(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{ \"content\": \"Hello, I want to bake a cake\",\"role\": \"user\"},\n",
    "                      { \"content\": \"Hello, I can pull up some recipes for cakes.\",\"role\": \"assistant\"},\n",
    "                      { \"content\": \"No actually I want to make a pie\",\"role\": \"user\"},],\n",
    "            temperature=0.7\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LiteLLMInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "litellm.embedding(model='text-embedding-ada-002', input=[\"good morning from litellm\"])\n",
    "await litellm.acompletion(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{ \"content\": \"Hello, I want to bake a cake\",\"role\": \"user\"},\n",
    "                      { \"content\": \"Hello, I can pull up some recipes for cakes.\",\"role\": \"assistant\"},\n",
    "                      { \"content\": \"No actually I want to make a pie\",\"role\": \"user\"},],\n",
    "            temperature=0.7,\n",
    "            max_tokens=20\n",
    "        )\n",
    "litellm.completion(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{ \"content\": \"Hello, I want to bake a cake\",\"role\": \"user\"},\n",
    "                      { \"content\": \"Hello, I can pull up some recipes for cakes.\",\"role\": \"assistant\"},\n",
    "                      { \"content\": \"No actually I want to make a pie\",\"role\": \"user\"},],\n",
    "            temperature=0.7\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openinference-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
