{"text": "Introduction\n============\n\n\n\u200bSince releasing the Classifications endpoint in beta last year, we\u2019ve developed new methods that achieve better results for this task. As a result, we\u2019ll be removing the Classifications endpoints from our documentation and removing access to this endpoint on December 3, 2022 for all organizations. New accounts created after June 3rd will not have access to this endpoint.\n\n\n\nWe strongly encourage developers to switch over to newer techniques which produce better results, outlined below.\n\n\n\nCurrent documentation\n---------------------\n\n\n<https://beta.openai.com/docs/guides/classifications> \n\n\n<https://beta.openai.com/docs/api-reference/classifications> \n\n\n\nOptions\n=======\n\n\nAll of these options are also outlined [here](https://github.com/openai/openai-cookbook/tree/main/transition_guides_for_deprecated_API_endpoints).\n\n\n\nAs a quick review, here are the high level steps of the current Classifications endpoint:\n\n\n\n\n![](https://openai.intercom-attachments-7.com/i/o/524219891/aa3136e9c7bcd8697c51ae9a/wDEz1wePRC3E7UyA1n0lsTPUvVakpPlMQ92SDnvEsScQFclIRW-bO2eKRhAp9_15j0vnyPYnhG71PjJj6Fttfwdpb1UnHZzMle9llSC76HQHN9lCzMNF6N2UDmeWzOldgwqRYYy-hzxBAD61Nw)\nOption 1: Transition to fine-tuning (recommended)\n-------------------------------------------------\n\n\nWe believe that most use cases will be better served by moving to a fine tuned model. The major reason for this is that our current system used a bigram filter to narrow down the scope of candidates whereas our fine tuned system can take in an arbitrary amount of data and learn more nuance between examples. For more on creating a fine tuned model, check out our [guide](https://beta.openai.com/docs/guides/fine-tuning/classification). \n\n\n\nOption 2: Transition to Embeddings-based search\n-----------------------------------------------\n\n\nAnother possible option, especially if your classification labels change frequently, is to use embeddings. If you\u2019re not familiar with this, you can learn more by visiting our [guide to embeddings](https://beta.openai.com/docs/guides/embeddings/use-cases).\n\n\n\nIf you\u2019re using a small dataset (<10,000 documents), consider using the techniques described in that guide to find the best documents to construct a prompt similar to [this](#h_e63b71a5c8). Then, you can just submit that prompt to our [Completions](https://beta.openai.com/docs/api-reference/completions) endpoint.\n\n\n\nIf you have a larger dataset, consider using a vector search engine like [Pinecone](https://share.streamlit.io/pinecone-io/playground/beyond_search_openai/src/server.py) or [Weaviate](https://weaviate.io/developers/weaviate/current/retriever-vectorizer-modules/text2vec-openai.html) to power that search.\n\n\n\nOption 3: Reimplement existing functionality\n--------------------------------------------\n\n\nIf you\u2019d like to recreate the functionality of the Classifications endpoint, here\u2019s how we did it. This functionality is also mostly replicated in this [script](https://github.com/openai/openai-cookbook/blob/main/transition_guides_for_deprecated_API_endpoints/classification_functionality_example.py).\n\n\n\nAt a high level, there are two main ways you can use the classifications endpoint: you can source the data from an uploaded file or send it in with the request.\n\n\n\nIf you're using the document parameter\n--------------------------------------\n\n\nThere\u2019s only one step if you provide the documents in the Classifications API call.\n\n\n\nHere\u2019s roughly the steps we used: \n\n\n* Construct the prompt [with this format.](#h_e63b71a5c8)\n* Gather all of the provided documents. If they fit in the prompt, just use all of them.\n* Do an [OpenAI search](https://beta.openai.com/docs/api-reference/searches) (also being deprecated. Please see its [transition guide](https://help.openai.com/en/articles/6272952-search-transition-guide)) where the documents are the user provided documents and the query is the query from above. Rank the documents by score.\n* In order of score, attempt to add Elastic search documents until you run out of space in the context. Try to maximize the number of distinct labels as that will help the model understand the different labels that are available.\n* Request a completion with the provided parameters (logit\\_bias, n, stop, etc)\n\nThroughout all of this, you\u2019ll need to check that the prompt\u2019s length doesn\u2019t exceed [the model's token limit](https://beta.openai.com/docs/engines/gpt-3). To assess the number of tokens present in a prompt, we recommend <https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2TokenizerFast>. \n\n\n\nIf you're using the file parameter\n----------------------------------\n\n\nStep 1: upload a jsonl file\n\n\n\nBehind the scenes, we upload new files meant for classifications to an Elastic search. Each line of the jsonl is then submitted as a document.\n\n\n\nIn each line we require a \u201ctext\u201d field, a \u201clabel\u201d field, and an optional \u201cmetadata\u201d field\n\n\n\nThese are the Elastic search settings and mappings for our index:\n\n\n\n[Elastic searching mapping](https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping.html): \n\n\n\n```\n{  \n    \"properties\": {  \n        \"document\": {\"type\": \"text\", \"analyzer\": \"standard_bigram_analyzer\"}, -> the \u201ctext\u201d field  \n        \"label\": {\"type\": \"text\", \"analyzer\": \"standard_bigram_analyzer\"},  \n        \"metadata\": {\"type\": \"object\", \"enabled\": False}, -> the \u201cmetadata\u201d field  \n    }  \n}\n```\n\n\n[Elastic search analyzer](https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping.html):\n\n\n\n```\n{  \n    \"analysis\": {  \n        \"analyzer\": {  \n            \"standard_bigram_analyzer\": {  \n                \"type\": \"custom\",  \n                \"tokenizer\": \"standard\",  \n                \"filter\": [\"lowercase\", \"english_stop\", \"shingle\"],  \n            }  \n        },  \n        \"filter\": {\"english_stop\": {\"type\": \"stop\", \"stopwords\": \"_english_\"}},  \n    }  \n}\n```\n\n\nAfter that, we performed [standard Elastic search search calls](https://elasticsearch-py.readthedocs.io/en/v8.2.0/api.html#elasticsearch.Elasticsearch.search) and used `max\\_examples` to determine the number of documents to return from Elastic search.\n\n\n\nStep 2: Search\n\n\nHere\u2019s roughly the steps we used. Our end goal is to create a [Completions](https://beta.openai.com/docs/api-reference/completions) request [with this format](#h_e63b71a5c8). It will look very similar to [Documents](#h_51fe4aed6d).\n\n\n\nFrom there, our steps are: \n\n\n* Start with the `experimental\\_alternative\\_question` or, if that's not provided, what\u2019s in the `question` field. Call that the query.\n* Query Elastic search for `max\\_examples` documents with query as the search param.\n* Take those documents and do an [OpenAI search](https://beta.openai.com/docs/api-reference/searches) on them where the entries from Elastic search are the docs, and the query is the query that you used above. Use the score from the search to rank the documents.\n* In order of score, attempt to add Elastic search documents until you run out of space in the prompt. Try to maximize the number of distinct labels as that will help the model understand the different labels that are available.\n* Request an OpenAI completion with the provided parameters (logit\\_bias, n, stop, etc). Return that generation to the user.\n\n\nCompletion Prompt\n-----------------\n\n\n\n```\n#{{ an optional instruction }}  \n  \nText: #{{example 1 text}}  \nCategory: #{{example 1 label}}  \n---  \nText: #{{example 2 text}}  \nCategory: #{{example 2 label}}  \n---  \nText: #{{question}}  \nCategory:\n```\n\n", "title": "Classifications Transition Guide", "article_id": "6272941", "url": "https://help.openai.com/en/articles/6272941-classifications-transition-guide"}