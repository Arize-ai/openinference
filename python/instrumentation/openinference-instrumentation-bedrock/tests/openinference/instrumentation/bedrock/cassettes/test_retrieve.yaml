interactions:
- request:
    body: '{"retrievalQuery": {"text": "What is task Decomposition?"}}'
    headers: {}
    method: POST
    uri: https://bedrock-agent-runtime.ap-south-1.amazonaws.com/knowledgebases/SSGLURQ9A5/retrieve
  response:
    body:
      string: "{\"retrievalResults\":[{\"content\":{\"text\":\"Fig. 1. Overview of
        a LLM-powered autonomous agent system. Component One: Planning A complicated
        task usually involves many steps. An agent needs to know what they are and
        plan ahead.  Task Decomposition Chain of thought (CoT; Wei et al. 2022) has
        become a standard prompting technique for enhancing model performance on complex
        tasks. The model is instructed to \u201Cthink step by step\u201D to utilize
        more test-time computation to decompose hard tasks into smaller and simpler
        steps. CoT transforms big tasks into multiple manageable tasks and shed lights
        into an interpretation of the model\u2019s thinking process.  Tree of Thoughts
        (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities
        at each step. It first decomposes the problem into multiple thought steps
        and generates multiple thoughts per step, creating a tree structure. The search
        process can be BFS (breadth-first search) or DFS (depth-first search) with
        each state evaluated by a classifier (via a prompt) or majority vote.  Task
        decomposition can be done (1) by LLM with simple prompting like \\\"Steps
        for XYZ.\\\\n1.\\\", \\\"What are the subgoals for achieving XYZ?\\\", (2)
        by using task-specific instructions; e.g. \\\"Write a story outline.\\\" for
        writing a novel, or (3) with human inputs.\",\"type\":\"TEXT\"},\"location\":{\"customDocumentLocation\":{\"id\":\"2222\"},\"type\":\"CUSTOM\"},\"metadata\":{\"x-amz-bedrock-kb-source-uri\":\"2222\",\"x-amz-bedrock-kb-data-source-id\":\"VYV3J5D9O6\"},\"score\":0.6244210600852966},{\"content\":{\"text\":\"They
        use few-shot examples to guide LLM to do task parsing and planning.  Instruction:
        \ The AI assistant can parse user input to several tasks: [{\\\"task\\\":
        task, \\\"id\\\", task_id, \\\"dep\\\": dependency_task_ids, \\\"args\\\":
        {\\\"text\\\": text, \\\"image\\\": URL, \\\"audio\\\": URL, \\\"video\\\":
        URL}}]. The \\\"dep\\\" field denotes the id of the previous task which generates
        a new resource that the current task relies on. A special tag \\\"-task_id\\\"
        refers to the generated text image, audio and video in the dependency task
        with id as task_id. The task MUST be selected from the following options:
        {{ Available Task List }}. There is a logical relationship between tasks,
        please note their order. If the user input can't be parsed, you need to reply
        empty JSON. Here are several cases for your reference: {{ Demonstrations }}.
        The chat history is recorded as {{ Chat History }}. From this chat history,
        you can find the path of the user-mentioned resources for your task planning.
        (2) Model selection: LLM distributes the tasks to expert models, where the
        request is framed as a multiple-choice question. LLM is presented with a list
        of models to choose from. Due to the limited context length, task type based
        filtration is needed.\",\"type\":\"TEXT\"},\"location\":{\"customDocumentLocation\":{\"id\":\"2222\"},\"type\":\"CUSTOM\"},\"metadata\":{\"x-amz-bedrock-kb-source-uri\":\"2222\",\"x-amz-bedrock-kb-data-source-id\":\"VYV3J5D9O6\"},\"score\":0.37011033296585083},{\"content\":{\"text\":\"The
        training dataset in their experiments is a combination of WebGPT comparisons,
        summarization from human feedback and human preference dataset.   Fig. 5.
        After fine-tuning with CoH, the model can follow instructions to produce outputs
        with incremental improvement in a sequence. (Image source: Liu et al. 2023)
        The idea of CoH is to present a history of sequentially improved outputs in
        context and train the model to take on the trend to produce better outputs.
        Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode
        trajectories in reinforcement learning tasks, where an algorithm is encapsulated
        in a long history-conditioned policy. Considering that an agent interacts
        with the environment many times and in each episode the agent gets a little
        better, AD concatenates this learning history and feeds that into the model.
        Hence we should expect the next predicted action to lead to better performance
        than previous trials. The goal is to learn the process of RL instead of training
        a task-specific policy itself.   Fig. 6. Illustration of how Algorithm Distillation
        (AD) works. (Image source: Laskin et al. 2023). The paper hypothesizes that
        any algorithm that generates a set of learning histories can be distilled
        into a neural network by performing behavioral cloning over actions. The history
        data is generated by a set of source policies, each trained for a specific
        task.\",\"type\":\"TEXT\"},\"location\":{\"customDocumentLocation\":{\"id\":\"2222\"},\"type\":\"CUSTOM\"},\"metadata\":{\"x-amz-bedrock-kb-source-uri\":\"2222\",\"x-amz-bedrock-kb-data-source-id\":\"VYV3J5D9O6\"},\"score\":0.2538279891014099},{\"content\":{\"text\":\"The
        code should be fully functional. Make sure that code in different files are
        compatible with each other.\\\\nBefore you finish, double check that all parts
        of the architecture is present in the files.\\\\n\\\"   } ] Challenges After
        going through key ideas and demos of building LLM-centered agents, I start
        to see a couple common limitations:  Finite context length: The restricted
        context capacity limits the inclusion of historical information, detailed
        instructions, API call context, and responses. The design of the system has
        to work with this limited communication bandwidth, while mechanisms like self-reflection
        to learn from past mistakes would benefit a lot from long or infinite context
        windows. Although vector stores and retrieval can provide access to a larger
        knowledge pool, their representation power is not as powerful as full attention.
        \ Challenges in long-term planning and task decomposition: Planning over a
        lengthy history and effectively exploring the solution space remain challenging.
        LLMs struggle to adjust plans when faced with unexpected errors, making them
        less robust compared to humans who learn from trial and error.  Reliability
        of natural language interface: Current agent system relies on natural language
        as an interface between LLMs and external components such as memory and tools.
        However, the reliability of model outputs is questionable, as LLMs may make
        formatting errors and occasionally exhibit rebellious behavior (e.g. refuse
        to follow an instruction). Consequently, much of the agent demo code focuses
        on parsing model output.  Citation Cited as:  Weng, Lilian.\",\"type\":\"TEXT\"},\"location\":{\"customDocumentLocation\":{\"id\":\"2222\"},\"type\":\"CUSTOM\"},\"metadata\":{\"x-amz-bedrock-kb-source-uri\":\"2222\",\"x-amz-bedrock-kb-data-source-id\":\"VYV3J5D9O6\"},\"score\":0.2205149084329605},{\"content\":{\"text\":\"LLM
        is presented with a list of models to choose from. Due to the limited context
        length, task type based filtration is needed.  Instruction:  Given the user
        request and the call command, the AI assistant helps the user to select a
        suitable model from a list of models to process the user request. The AI assistant
        merely outputs the model id of the most appropriate model. The output must
        be in a strict JSON format: \\\"id\\\": \\\"id\\\", \\\"reason\\\": \\\"your
        detail reason for the choice\\\". We have a list of models for you to choose
        from {{ Candidate Models }}. Please select one model from the list. (3) Task
        execution: Expert models execute on the specific tasks and log results.  Instruction:
        \ With the input and the inference results, the AI assistant needs to describe
        the process and results. The previous stages can be formed as - User Input:
        {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment
        }}, Task Execution: {{ Predictions }}. You must first answer the user's request
        in a straightforward manner. Then describe the task process and show your
        analysis and model inference results to the user in the first person. If inference
        results contain a file path, must tell the user the complete file path. (4)
        Response generation: LLM receives the execution results and provides summarized
        results to users.\",\"type\":\"TEXT\"},\"location\":{\"customDocumentLocation\":{\"id\":\"2222\"},\"type\":\"CUSTOM\"},\"metadata\":{\"x-amz-bedrock-kb-source-uri\":\"2222\",\"x-amz-bedrock-kb-data-source-id\":\"VYV3J5D9O6\"},\"score\":0.18180496990680695}]}"
    headers: {}
    status:
      code: 200
      message: OK
version: 1
