interactions:
- request:
    body: '{"contents": [{"parts": [{"text": "What''s the weather like in San Francisco?"}],
      "role": "user"}], "systemInstruction": {"parts": [{"text": "You are a helpful
      assistant that can answer questions and help with tasks. Use the available tools
      when appropriate."}], "role": "user"}, "tools": [{"functionDeclarations": [{"description":
      "\n        This function will be automatically executed by Google GenAI.\n        It
      must return a JSON-serializable value.\n        ", "name": "get_weather", "parameters":
      {"properties": {"location": {"type": "STRING"}, "unit": {"default": "fahrenheit",
      "type": "STRING"}}, "required": ["location"], "type": "OBJECT"}}]}], "generationConfig":
      {}}'
    headers: {}
    method: POST
    uri: https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent
  response:
    body:
      string: "{\n  \"candidates\": [\n    {\n      \"content\": {\n        \"parts\":
        [\n          {\n            \"functionCall\": {\n              \"name\": \"get_weather\",\n
        \             \"args\": {\n                \"location\": \"San Francisco\"\n
        \             }\n            }\n          }\n        ],\n        \"role\":
        \"model\"\n      },\n      \"finishReason\": \"STOP\",\n      \"avgLogprobs\":
        -0.061545997858047485\n    }\n  ],\n  \"usageMetadata\": {\n    \"promptTokenCount\":
        69,\n    \"candidatesTokenCount\": 6,\n    \"totalTokenCount\": 75,\n    \"promptTokensDetails\":
        [\n      {\n        \"modality\": \"TEXT\",\n        \"tokenCount\": 69\n
        \     }\n    ],\n    \"candidatesTokensDetails\": [\n      {\n        \"modality\":
        \"TEXT\",\n        \"tokenCount\": 6\n      }\n    ]\n  },\n  \"modelVersion\":
        \"gemini-2.0-flash\",\n  \"responseId\": \"DE4_aI-DAfyTmNAP44qv0AU\"\n}\n"
    headers: {}
    status:
      code: 200
      message: OK
- request:
    body: '{"contents": [{"parts": [{"text": "What''s the weather like in San Francisco?"}],
      "role": "user"}, {"parts": [{"functionCall": {"args": {"location": "San Francisco"},
      "name": "get_weather"}}], "role": "model"}, {"parts": [{"functionResponse":
      {"name": "get_weather", "response": {"result": {"location": "San Francisco",
      "temperature": 65, "unit": "fahrenheit", "condition": "foggy", "humidity": "85%"}}}}],
      "role": "user"}], "systemInstruction": {"parts": [{"text": "You are a helpful
      assistant that can answer questions and help with tasks. Use the available tools
      when appropriate."}], "role": "user"}, "tools": [{"functionDeclarations": [{"description":
      "\n        This function will be automatically executed by Google GenAI.\n        It
      must return a JSON-serializable value.\n        ", "name": "get_weather", "parameters":
      {"properties": {"location": {"type": "STRING"}, "unit": {"default": "fahrenheit",
      "type": "STRING"}}, "required": ["location"], "type": "OBJECT"}}]}], "generationConfig":
      {}}'
    headers: {}
    method: POST
    uri: https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent
  response:
    body:
      string: "{\n  \"candidates\": [\n    {\n      \"content\": {\n        \"parts\":
        [\n          {\n            \"text\": \"The weather in San Francisco is foggy
        with a temperature of 65 degrees Fahrenheit and 85% humidity.\\n\"\n          }\n
        \       ],\n        \"role\": \"model\"\n      },\n      \"finishReason\":
        \"STOP\",\n      \"avgLogprobs\": -0.0065877387921015424\n    }\n  ],\n  \"usageMetadata\":
        {\n    \"promptTokenCount\": 94,\n    \"candidatesTokenCount\": 24,\n    \"totalTokenCount\":
        118,\n    \"promptTokensDetails\": [\n      {\n        \"modality\": \"TEXT\",\n
        \       \"tokenCount\": 94\n      }\n    ],\n    \"candidatesTokensDetails\":
        [\n      {\n        \"modality\": \"TEXT\",\n        \"tokenCount\": 24\n
        \     }\n    ]\n  },\n  \"modelVersion\": \"gemini-2.0-flash\",\n  \"responseId\":
        \"DE4_aOa4KfyTmNAP44qv0AU\"\n}\n"
    headers: {}
    status:
      code: 200
      message: OK
version: 1
