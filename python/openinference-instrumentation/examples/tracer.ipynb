{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict\n",
    "\n",
    "import pydantic\n",
    "from opentelemetry.trace import Status, StatusCode, get_current_span\n",
    "\n",
    "from openinference.instrumentation import (\n",
    "    get_input_attributes,\n",
    "    get_output_attributes,\n",
    "    get_span_kind_attributes,\n",
    "    get_tool_attributes,\n",
    "    suppress_tracing,\n",
    "    using_attributes,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either instrument with `TracerProvider` from `openinference.instrumentation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
    "from opentelemetry.sdk.resources import Resource\n",
    "from opentelemetry.sdk.trace.export import ConsoleSpanExporter, SimpleSpanProcessor\n",
    "\n",
    "from openinference.instrumentation import TracerProvider\n",
    "from openinference.semconv.resource import ResourceAttributes\n",
    "\n",
    "endpoint = \"http://127.0.0.1:6006/v1/traces\"\n",
    "resource = Resource(attributes={ResourceAttributes.PROJECT_NAME: \"openinference-tracer\"})\n",
    "tracer_provider = TracerProvider(resource=resource)\n",
    "tracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter(endpoint)))\n",
    "tracer_provider.add_span_processor(SimpleSpanProcessor(ConsoleSpanExporter()))\n",
    "tracer = tracer_provider.get_tracer(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or using `phoenix.otel.register` (in which case, comment out cell above and uncomment this cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from phoenix.otel import register\n",
    "\n",
    "# tracer_provider = register(protocol=\"http/protobuf\")\n",
    "# tracer = tracer_provider.get_tracer(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from anthropic import Anthropic\n",
    "from openai import OpenAI\n",
    "from openai.types.chat import (\n",
    "    ChatCompletionMessageParam,\n",
    "    ChatCompletionSystemMessageParam,\n",
    "    ChatCompletionToolMessageParam,\n",
    "    ChatCompletionToolParam,\n",
    "    ChatCompletionUserMessageParam,\n",
    ")\n",
    "\n",
    "anthropic_client = Anthropic()\n",
    "openai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tracer.start_as_current_span(\"llm-span\", openinference_span_kind=\"llm\") as span:\n",
    "    invocation_parameters = {\n",
    "        \"temperature\": 0.5,\n",
    "    }\n",
    "    input_messages: List[ChatCompletionMessageParam] = [\n",
    "        ChatCompletionSystemMessageParam(\n",
    "            role=\"system\",\n",
    "            content=\"You are a helpful assistant.\",\n",
    "        ),\n",
    "        ChatCompletionUserMessageParam(\n",
    "            role=\"user\",\n",
    "            content=\"Hello, world!\",\n",
    "        ),\n",
    "    ]\n",
    "    span.set_llm(\n",
    "        provider=\"openai\",\n",
    "        system=\"openai\",\n",
    "        input_messages=input_messages,\n",
    "        model_name=\"gpt-4o\",\n",
    "        invocation_parameters=invocation_parameters,\n",
    "    )\n",
    "    span.set_input(input_messages[-1][\"content\"])\n",
    "    message = openai_client.chat.completions.create(  # type: ignore[call-overload]\n",
    "        messages=input_messages,\n",
    "        model=\"gpt-4o\",\n",
    "        **invocation_parameters,\n",
    "    )\n",
    "    output_message = message.choices[0].message\n",
    "    assert (token_usage := message.usage) is not None\n",
    "    assert isinstance(completion_tokens := token_usage.completion_tokens, int)\n",
    "    assert isinstance(prompt_tokens := token_usage.prompt_tokens, int)\n",
    "    span.set_status(Status(StatusCode.OK))\n",
    "    span.set_llm(\n",
    "        output_messages=[\n",
    "            {\n",
    "                \"role\": output_message.role,\n",
    "                \"content\": output_message.content,\n",
    "            }\n",
    "        ],\n",
    "        token_count={\n",
    "            \"completion\": completion_tokens,\n",
    "            \"prompt\": prompt_tokens,\n",
    "        },\n",
    "    )\n",
    "    span.set_output(output_message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tracer.start_as_current_span(\"llm-span-tool-calls\", openinference_span_kind=\"llm\") as span:\n",
    "    input_messages = [\n",
    "        ChatCompletionUserMessageParam(\n",
    "            role=\"user\",\n",
    "            content=\"What's the weather like in San Francisco?\",\n",
    "        )\n",
    "    ]\n",
    "    invocation_parameters = {\n",
    "        \"temperature\": 0.5,\n",
    "    }\n",
    "    tools: List[ChatCompletionToolParam] = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_weather\",\n",
    "                \"description\": \"finds the weather for a given city\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"city\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city to find the weather for, e.g. 'London'\",\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"city\"],\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    ]\n",
    "    span.set_llm(\n",
    "        provider=\"openai\",\n",
    "        system=\"openai\",\n",
    "        input_messages=input_messages,\n",
    "        model_name=\"gpt-4o\",\n",
    "        invocation_parameters=invocation_parameters,\n",
    "    )\n",
    "    span.set_input(input_messages[-1][\"content\"])\n",
    "    message = openai_client.chat.completions.create(  # type: ignore[call-overload]\n",
    "        model=\"gpt-4\",\n",
    "        tools=tools,\n",
    "        messages=input_messages,\n",
    "        **invocation_parameters,\n",
    "    )\n",
    "    span.set_status(Status(StatusCode.OK))\n",
    "    output_message = message.choices[0].message\n",
    "    assert (token_usage := message.usage) is not None\n",
    "    assert isinstance(completion_tokens := token_usage.completion_tokens, int)\n",
    "    assert isinstance(prompt_tokens := token_usage.prompt_tokens, int)\n",
    "    assert (tool_calls := output_message.tool_calls)\n",
    "    assert len(tool_calls) == 1\n",
    "    tool_call = tool_calls[0]\n",
    "    span.set_llm(\n",
    "        output_messages=[\n",
    "            {\n",
    "                \"role\": output_message.role,\n",
    "                \"content\": output_message.content,\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": tool_call.id,\n",
    "                        \"function\": {\n",
    "                            \"name\": tool_call.function.name,\n",
    "                            \"arguments\": tool_call.function.arguments,\n",
    "                        },\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        token_count={\n",
    "            \"completion\": completion_tokens,\n",
    "            \"prompt\": prompt_tokens,\n",
    "        },\n",
    "    )\n",
    "    span.set_output(f\"{tool_call.function.name}({tool_call.function.arguments})\")\n",
    "\n",
    "with tracer.start_as_current_span(\"llm-span-content-blocks\", openinference_span_kind=\"llm\") as span:\n",
    "    input_messages.append(output_message.model_dump())\n",
    "    input_messages.append(\n",
    "        ChatCompletionToolMessageParam(content=\"sunny\", role=\"tool\", tool_call_id=tool_call.id)\n",
    "    )\n",
    "    span.set_llm(\n",
    "        provider=\"openai\",\n",
    "        system=\"openai\",\n",
    "        input_messages=input_messages,\n",
    "        model_name=\"gpt-4o\",\n",
    "        invocation_parameters=invocation_parameters,\n",
    "    )\n",
    "    span.set_input(input_messages[-1][\"content\"])\n",
    "    message = openai_client.chat.completions.create(  # type: ignore[call-overload]\n",
    "        model=\"gpt-4\",\n",
    "        tools=tools,\n",
    "        messages=input_messages,\n",
    "        **invocation_parameters,\n",
    "    )\n",
    "    span.set_status(Status(StatusCode.OK))\n",
    "    output_message = message.choices[0].message\n",
    "    assert (token_usage := message.usage) is not None\n",
    "    assert isinstance(completion_tokens := token_usage.completion_tokens, int)\n",
    "    assert isinstance(prompt_tokens := token_usage.prompt_tokens, int)\n",
    "    span.set_status(Status(StatusCode.OK))\n",
    "    span.set_llm(\n",
    "        output_messages=[\n",
    "            {\n",
    "                \"role\": output_message.role,\n",
    "                \"content\": output_message.content,\n",
    "            }\n",
    "        ],\n",
    "        token_count={\n",
    "            \"completion\": completion_tokens,\n",
    "            \"prompt\": prompt_tokens,\n",
    "        },\n",
    "    )\n",
    "    span.set_output(output_message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tracer.start_as_current_span(\"llm-span-content-blocks\", openinference_span_kind=\"llm\") as span:\n",
    "    invocation_parameters = {\n",
    "        \"max_tokens\": 100,\n",
    "    }\n",
    "    span.set_llm(\n",
    "        provider=\"anthropic\",\n",
    "        system=\"anthropic\",\n",
    "        input_messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"contents\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"image\": {\n",
    "                            \"url\": \"https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg\",\n",
    "                        },\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"What is in the above image?\",\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        model_name=\"gpt-4o\",\n",
    "        invocation_parameters=invocation_parameters,\n",
    "    )\n",
    "    span.set_input(\"What is in the above image?\")\n",
    "    message = anthropic_client.messages.create(  # type: ignore[call-overload]\n",
    "        model=\"claude-3-5-sonnet-20240620\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"url\",\n",
    "                            \"url\": \"https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg\",\n",
    "                        },\n",
    "                    },\n",
    "                    {\"type\": \"text\", \"text\": \"What is in the above image?\"},\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        **invocation_parameters,\n",
    "    )\n",
    "    span.set_status(Status(StatusCode.OK))\n",
    "    token_usage = message.usage\n",
    "    prompt_tokens = token_usage.input_tokens\n",
    "    completion_tokens = token_usage.output_tokens\n",
    "    output_message = message.content[0]\n",
    "    assert output_message.type == \"text\"\n",
    "    output_message_text = output_message.text\n",
    "    span.set_llm(\n",
    "        output_messages=[\n",
    "            {\n",
    "                \"role\": message.role,\n",
    "                \"content\": output_message_text,\n",
    "            }\n",
    "        ],\n",
    "        token_count={\n",
    "            \"completion\": completion_tokens,\n",
    "            \"prompt\": prompt_tokens,\n",
    "        },\n",
    "    )\n",
    "    span.set_output(output_message_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic.types import Message as AnthropicMessage\n",
    "from anthropic.types import MessageParam as AnthropicMessageParam\n",
    "\n",
    "from openinference.instrumentation import get_llm_attributes\n",
    "from openinference.instrumentation._types import (\n",
    "    Image as OIImage,\n",
    ")\n",
    "from openinference.instrumentation._types import (\n",
    "    ImageMessageContent as OIImageMessageContent,\n",
    ")\n",
    "from openinference.instrumentation._types import (\n",
    "    Message as OIMessage,\n",
    ")\n",
    "from openinference.instrumentation._types import (\n",
    "    MessageContent as OIMessageContent,\n",
    ")\n",
    "from openinference.instrumentation._types import (\n",
    "    TextMessageContent as OITextMessageContent,\n",
    ")\n",
    "\n",
    "\n",
    "def get_attributes_from_inputs(\n",
    "    input_messages: List[AnthropicMessageParam],\n",
    "    invocation_parameters: Dict[str, Any],\n",
    ") -> Dict[str, Any]:\n",
    "    oi_input_messages = [to_oi_message(message) for message in input_messages]\n",
    "    return {\n",
    "        **get_input_attributes(\n",
    "            {\n",
    "                \"input_messages\": input_messages,\n",
    "                \"invocation_parameters\": invocation_parameters,\n",
    "            }\n",
    "        ),\n",
    "        **get_llm_attributes(\n",
    "            input_messages=oi_input_messages,\n",
    "            invocation_parameters=invocation_parameters,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "def to_oi_message(message: AnthropicMessageParam) -> OIMessage:\n",
    "    role = message[\"role\"]\n",
    "    content = message[\"content\"]\n",
    "    if isinstance(content, str):\n",
    "        return OIMessage(role=role, content=content)\n",
    "\n",
    "    contents: List[OIMessageContent] = []\n",
    "    for content_block in content:\n",
    "        if not isinstance(content_block, dict):\n",
    "            raise NotImplementedError(\"Only typed dict message params are supported\")\n",
    "        if (content_type := content_block[\"type\"]) == \"text\":\n",
    "            assert isinstance(text := content_block.get(\"text\"), str)\n",
    "            contents.append(OITextMessageContent(type=\"text\", text=text))\n",
    "        elif content_type == \"image\":\n",
    "            assert isinstance(source := content_block.get(\"source\"), dict)\n",
    "            assert isinstance(url := source.get(\"url\"), str)\n",
    "            contents.append(\n",
    "                OIImageMessageContent(\n",
    "                    type=\"image\",\n",
    "                    image=OIImage(url=url),\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(\"Only text and image message content blocks are supported\")\n",
    "    return OIMessage(role=role, contents=contents)\n",
    "\n",
    "\n",
    "def get_attributes_from_outputs(message: AnthropicMessage) -> Dict[str, Any]:\n",
    "    assert len(message.content) == 1\n",
    "    content_block = message.content[0]\n",
    "    assert content_block.type == \"text\"\n",
    "    content_block_text = content_block.text\n",
    "    return {\n",
    "        **get_llm_attributes(\n",
    "            output_messages=[\n",
    "                {\n",
    "                    \"role\": message.role,\n",
    "                    \"content\": content_block_text,\n",
    "                }\n",
    "            ],\n",
    "            token_count={\n",
    "                \"completion\": message.usage.output_tokens,\n",
    "                \"prompt\": message.usage.input_tokens,\n",
    "            },\n",
    "        ),\n",
    "        **get_output_attributes(content_block_text),\n",
    "    }\n",
    "\n",
    "\n",
    "@tracer.llm(\n",
    "    get_attributes_from_inputs=get_attributes_from_inputs,\n",
    "    get_attributes_from_outputs=get_attributes_from_outputs,\n",
    ")\n",
    "def get_image_description(\n",
    "    input_messages: List[AnthropicMessageParam], invocation_parameters: Dict[str, Any]\n",
    ") -> AnthropicMessage:\n",
    "    output_message = anthropic_client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20240620\",\n",
    "        messages=input_messages,\n",
    "        **invocation_parameters,\n",
    "    )\n",
    "    return output_message\n",
    "\n",
    "\n",
    "response = get_image_description(\n",
    "    input_messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"source\": {\n",
    "                        \"type\": \"url\",\n",
    "                        \"url\": \"https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg\",\n",
    "                    },\n",
    "                },\n",
    "                {\"type\": \"text\", \"text\": \"What is in the above image?\"},\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    invocation_parameters={\"temperature\": 0.5, \"max_tokens\": 100},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tracer.start_as_current_span(\n",
    "    \"chain-span-with-plain-text-io\",\n",
    "    openinference_span_kind=\"chain\",\n",
    ") as span:\n",
    "    span.set_input(\"input\")\n",
    "    span.set_output(\"output\")\n",
    "    span.set_status(Status(StatusCode.OK))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tracer.start_as_current_span(\n",
    "    \"chain-span-with-json-io\",\n",
    "    openinference_span_kind=\"chain\",\n",
    ") as span:\n",
    "    span.set_input(\n",
    "        {\"input-key\": \"input-value\"},\n",
    "    )\n",
    "    span.set_output(\n",
    "        json.dumps({\"output-key\": \"output-value\"}),\n",
    "        mime_type=\"application/json\",\n",
    "    )\n",
    "    span.set_status(Status(StatusCode.OK))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tracer.start_as_current_span(\n",
    "    \"chain-span-with-attribute-getters\",\n",
    "    attributes={\n",
    "        **get_span_kind_attributes(\"chain\"),\n",
    "        **get_input_attributes(\"input\"),\n",
    "    },\n",
    ") as span:\n",
    "    span.set_attributes(get_output_attributes(\"output\"))\n",
    "    span.set_status(Status(StatusCode.OK))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputModel(pydantic.BaseModel):\n",
    "    input: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class OutputModel:\n",
    "    output: str\n",
    "\n",
    "\n",
    "with tracer.start_as_current_span(\n",
    "    \"chain-span-with-pydantic-input-and-dataclass-output\",\n",
    "    openinference_span_kind=\"chain\",\n",
    ") as span:\n",
    "    span.set_input(InputModel(input=\"input\"))\n",
    "    span.set_output(OutputModel(output=\"output\"))\n",
    "    span.set_status(Status(StatusCode.OK))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracer.chain\n",
    "def decorated_chain_with_plain_text_output(input: str) -> str:\n",
    "    return \"output\"\n",
    "\n",
    "\n",
    "decorated_chain_with_plain_text_output(\"input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracer.chain\n",
    "def decorated_chain_with_json_output(input: str) -> Dict[str, Any]:\n",
    "    return {\"output\": \"output\"}\n",
    "\n",
    "\n",
    "decorated_chain_with_json_output(\"input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracer.chain()\n",
    "def decorated_chain_with_no_parameters(input: str) -> Dict[str, Any]:\n",
    "    return {\"output\": \"output\"}\n",
    "\n",
    "\n",
    "decorated_chain_with_no_parameters(\"input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracer.chain(name=\"decorated-chain-with-overriden-name\")\n",
    "def this_name_should_be_overriden(input: str) -> Dict[str, Any]:\n",
    "    return {\"output\": \"output\"}\n",
    "\n",
    "\n",
    "this_name_should_be_overriden(\"input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_with_decorator_applied_as_function(input: str) -> Dict[str, Any]:\n",
    "    return {\"output\": \"output\"}\n",
    "\n",
    "\n",
    "decorated = tracer.chain(chain_with_decorator_applied_as_function)\n",
    "decorated(\"input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def this_name_should_be_overriden_with_decorator_applied_as_function_with_parameters(\n",
    "    input: str,\n",
    ") -> Dict[str, Any]:\n",
    "    return {\"output\": \"output\"}\n",
    "\n",
    "\n",
    "decorated = tracer.chain(\n",
    "    name=\"decorated-chain-with-decorator-applied-as-function-with-overriden-name\"\n",
    ")(this_name_should_be_overriden_with_decorator_applied_as_function_with_parameters)\n",
    "decorated(\"input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracer.chain\n",
    "async def decorated_async_chain(input: str) -> str:\n",
    "    return \"output\"\n",
    "\n",
    "\n",
    "await decorated_async_chain(\"input\")  # type: ignore[top-level-await]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracer.chain\n",
    "def decorated_chain_with_error(input: str) -> str:\n",
    "    raise ValueError(\"error\")\n",
    "\n",
    "\n",
    "try:\n",
    "    decorated_chain_with_error(\"input\")\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracer.chain\n",
    "def decorated_chain_with_child_span(input: str) -> str:\n",
    "    with tracer.start_as_current_span(\n",
    "        \"child-span\",\n",
    "        openinference_span_kind=\"chain\",\n",
    "        attributes=get_input_attributes(\"child-span-input\"),\n",
    "    ) as child_span:\n",
    "        output = \"output\"\n",
    "        child_span.set_output(output)\n",
    "        child_span.set_status(Status(StatusCode.OK))\n",
    "        return output\n",
    "\n",
    "\n",
    "decorated_chain_with_child_span(\"input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracer.chain\n",
    "def decorated_chain_with_child_span_error(input: str) -> str:\n",
    "    with tracer.start_as_current_span(\n",
    "        \"child-span\",\n",
    "        openinference_span_kind=\"chain\",\n",
    "        attributes=get_input_attributes(\"child-span-input\"),\n",
    "    ):\n",
    "        raise ValueError(\"error\")\n",
    "\n",
    "\n",
    "try:\n",
    "    decorated_chain_with_child_span_error(\"input\")\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChainRunner:\n",
    "    @tracer.chain\n",
    "    def decorated_chain_method(self, input1: str, input2: str) -> str:\n",
    "        return \"output\"\n",
    "\n",
    "\n",
    "chain_runner = ChainRunner()\n",
    "chain_runner.decorated_chain_method(\"input1\", \"input2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracer.chain\n",
    "def decorated_chain_with_input_and_output_set_inside_the_wrapped_function(input: str) -> str:\n",
    "    span = get_current_span()\n",
    "    span.set_input(\"overridden-input\")  # type: ignore[attr-defined]\n",
    "    span.set_output(\"overridden-output\")  # type: ignore[attr-defined]\n",
    "    return \"output\"\n",
    "\n",
    "\n",
    "decorated_chain_with_input_and_output_set_inside_the_wrapped_function(\"input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppress Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with suppress_tracing():\n",
    "    with tracer.start_as_current_span(\n",
    "        \"THIS-SPAN-SHOULD-NOT-BE-TRACED\",\n",
    "        openinference_span_kind=\"chain\",\n",
    "    ) as span:\n",
    "        span.set_input(\"input\")\n",
    "        span.set_output(\"output\")\n",
    "        span.set_status(Status(StatusCode.OK))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracer.chain\n",
    "def decorated_chain_with_suppress_tracing(input: str) -> str:\n",
    "    return \"output\"\n",
    "\n",
    "\n",
    "with suppress_tracing():\n",
    "    decorated_chain_with_suppress_tracing(\"input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with using_attributes(session_id=\"123\"):\n",
    "    with tracer.start_as_current_span(\n",
    "        \"chain-span-with-context-attributes\",\n",
    "        openinference_span_kind=\"chain\",\n",
    "    ) as span:\n",
    "        span.set_input(\"input\")\n",
    "        span.set_output(\"output\")\n",
    "        span.set_status(Status(StatusCode.OK))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracer.chain\n",
    "def decorated_chain_with_context_attributes(input: str) -> str:\n",
    "    return \"output\"\n",
    "\n",
    "\n",
    "with using_attributes(session_id=\"123\"):\n",
    "    decorated_chain_with_context_attributes(\"input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Managers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tracer.start_as_current_span(\n",
    "    \"agent-span-with-plain-text-io\",\n",
    "    openinference_span_kind=\"agent\",\n",
    ") as span:\n",
    "    span.set_input(\"input\")\n",
    "    span.set_output(\"output\")\n",
    "    span.set_status(Status(StatusCode.OK))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decorators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracer.agent\n",
    "def decorated_agent(input: str) -> str:\n",
    "    return \"output\"\n",
    "\n",
    "\n",
    "decorated_agent(\"input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Managers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tracer.start_as_current_span(\n",
    "    \"tool-span\",\n",
    "    openinference_span_kind=\"tool\",\n",
    ") as span:\n",
    "    span.set_input(\"input\")\n",
    "    span.set_output(\"output\")\n",
    "    span.set_tool(\n",
    "        name=\"tool-name\",\n",
    "        description=\"tool-description\",\n",
    "        parameters={\"input\": \"input\"},\n",
    "    )\n",
    "    span.set_status(Status(StatusCode.OK))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tracer.start_as_current_span(\n",
    "    \"tool-span-with-getter\",\n",
    "    openinference_span_kind=\"tool\",\n",
    ") as span:\n",
    "    span.set_attributes(\n",
    "        get_tool_attributes(\n",
    "            name=\"tool-name\",\n",
    "            description=\"tool-description\",\n",
    "            parameters={\"input\": \"input\"},\n",
    "        )\n",
    "    )\n",
    "    span.set_status(Status(StatusCode.OK))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracer.tool\n",
    "def decorated_tool(input1: str, input2: int) -> None:\n",
    "    \"\"\"\n",
    "    tool-description\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "decorated_tool(\"input1\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracer.tool\n",
    "async def decorated_tool_async(input1: str, input2: int) -> None:\n",
    "    \"\"\"\n",
    "    tool-description\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "await decorated_tool_async(\"input1\", 1)  # type: ignore[top-level-await]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracer.tool(\n",
    "    name=\"decorated-tool-with-overriden-name\",\n",
    "    description=\"overriden-tool-description\",\n",
    ")\n",
    "def this_tool_name_should_be_overriden(input1: str, input2: int) -> None:\n",
    "    \"\"\"\n",
    "    this tool description should be overriden\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "this_tool_name_should_be_overriden(\"input1\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracer.tool\n",
    "def tool_with_changes_inside_the_wrapped_function(input1: str, input2: int) -> str:\n",
    "    span = get_current_span()\n",
    "    print(type(span))\n",
    "    span.set_input(\"inside-input\")  # type: ignore[attr-defined]\n",
    "    span.set_output(\"inside-output\")  # type: ignore[attr-defined]\n",
    "    span.set_tool(  # type: ignore[attr-defined]\n",
    "        name=\"inside-tool-name\",\n",
    "        description=\"inside-tool-description\",\n",
    "        parameters={\"inside-input\": \"inside-input\"},\n",
    "    )\n",
    "    return \"output\"\n",
    "\n",
    "\n",
    "tool_with_changes_inside_the_wrapped_function(\"input1\", 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
