{
  "log": {
    "_recordingName": "bedrock-agent-rag - bedrockagent-rag-instrumentation---attributes-and-api-recording-should-record-retrieve-attributes-and-api-response-in-span",
    "creator": {
      "comment": "persister:fs",
      "name": "Polly.JS",
      "version": "6.0.6"
    },
    "entries": [
      {
        "_id": "8d00144bde76c8fe42715bb5fa766589",
        "_order": 0,
        "cache": {},
        "request": {
          "bodySize": 57,
          "cookies": [],
          "headers": [],
          "headersSize": 904,
          "httpVersion": "HTTP/1.1",
          "method": "POST",
          "postData": {
            "mimeType": "application/json",
            "params": [],
            "text": "{\"retrievalQuery\":{\"text\":\"What is Task Decomposition?\"}}"
          },
          "queryString": [],
          "url": "https://bedrock-agent-runtime.ap-south-1.amazonaws.com/knowledgebases/SSGLURQ9A5/retrieve"
        },
        "response": {
          "bodySize": 7663,
          "content": {
            "mimeType": "application/json",
            "size": 7663,
            "text": "{\"retrievalResults\":[{\"content\":{\"text\":\"Fig. 1. Overview of a LLM-powered autonomous agent system. Component One: Planning A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.  Task Decomposition Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.  Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.  Task decomposition can be done (1) by LLM with simple prompting like \\\"Steps for XYZ.\\\\n1.\\\", \\\"What are the subgoals for achieving XYZ?\\\", (2) by using task-specific instructions; e.g. \\\"Write a story outline.\\\" for writing a novel, or (3) with human inputs.\",\"type\":\"TEXT\"},\"location\":{\"customDocumentLocation\":{\"id\":\"2222\"},\"type\":\"CUSTOM\"},\"metadata\":{\"x-amz-bedrock-kb-source-uri\":\"2222\",\"x-amz-bedrock-kb-data-source-id\":\"VYV3J5D9O6\"},\"score\":0.6300203204154968},{\"content\":{\"text\":\"They use few-shot examples to guide LLM to do task parsing and planning.  Instruction:  The AI assistant can parse user input to several tasks: [{\\\"task\\\": task, \\\"id\\\", task_id, \\\"dep\\\": dependency_task_ids, \\\"args\\\": {\\\"text\\\": text, \\\"image\\\": URL, \\\"audio\\\": URL, \\\"video\\\": URL}}]. The \\\"dep\\\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \\\"-task_id\\\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can't be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning. (2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\",\"type\":\"TEXT\"},\"location\":{\"customDocumentLocation\":{\"id\":\"2222\"},\"type\":\"CUSTOM\"},\"metadata\":{\"x-amz-bedrock-kb-source-uri\":\"2222\",\"x-amz-bedrock-kb-data-source-id\":\"VYV3J5D9O6\"},\"score\":0.37894466519355774},{\"content\":{\"text\":\"The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.   Fig. 5. After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023) The idea of CoH is to present a history of sequentially improved outputs in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.   Fig. 6. Illustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023). The paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task.\",\"type\":\"TEXT\"},\"location\":{\"customDocumentLocation\":{\"id\":\"2222\"},\"type\":\"CUSTOM\"},\"metadata\":{\"x-amz-bedrock-kb-source-uri\":\"2222\",\"x-amz-bedrock-kb-data-source-id\":\"VYV3J5D9O6\"},\"score\":0.25131756067276},{\"content\":{\"text\":\"The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\"   } ] Challenges After going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:  Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.  Challenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.  Reliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.  Citation Cited as:  Weng, Lilian.\",\"type\":\"TEXT\"},\"location\":{\"customDocumentLocation\":{\"id\":\"2222\"},\"type\":\"CUSTOM\"},\"metadata\":{\"x-amz-bedrock-kb-source-uri\":\"2222\",\"x-amz-bedrock-kb-data-source-id\":\"VYV3J5D9O6\"},\"score\":0.22575193643569946},{\"content\":{\"text\":\"Consequently, much of the agent demo code focuses on parsing model output.  Citation Cited as:  Weng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.  Or  @article{weng2023agent,   title   = \\\"LLM-powered Autonomous Agents\\\",   author  = \\\"Weng, Lilian\\\",   journal = \\\"lilianweng.github.io\\\",   year    = \\\"2023\\\",   month   = \\\"Jun\\\",   url     = \\\"https://lilianweng.github.io/posts/2023-06-23-agent/\\\" } References [1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022  [2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).  [3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback “ arXiv preprint arXiv:2302.02676 (2023).  [4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\",\"type\":\"TEXT\"},\"location\":{\"customDocumentLocation\":{\"id\":\"2222\"},\"type\":\"CUSTOM\"},\"metadata\":{\"x-amz-bedrock-kb-source-uri\":\"2222\",\"x-amz-bedrock-kb-data-source-id\":\"VYV3J5D9O6\"},\"score\":0.18634532392024994}]}"
          },
          "cookies": [],
          "headers": [],
          "headersSize": 173,
          "httpVersion": "HTTP/1.1",
          "redirectURL": "",
          "status": 200,
          "statusText": "OK"
        },
        "startedDateTime": "2025-08-20T18:44:46.815Z",
        "time": 1810,
        "timings": {
          "blocked": -1,
          "connect": -1,
          "dns": -1,
          "receive": 0,
          "send": 0,
          "ssl": -1,
          "wait": 1810
        }
      }
    ],
    "pages": [],
    "version": "1.2"
  }
}
